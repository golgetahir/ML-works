{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = pd.read_csv('question-4-train-features.csv')\n",
    "dfTrainLabel = pd.read_csv('question-4-train-labels.csv')\n",
    "dfTestFeature = pd.read_csv('question-4-test-features.csv')\n",
    "dfTestLabel = pd.read_csv('question-4-test-labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    7091\n",
       "neutral     2616\n",
       "positive    2004\n",
       "Name: neutral, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrainLabel['neutral'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    def __init__(self):\n",
    "        self.p_of_positive = [] \n",
    "        self.p_of_negative = [] \n",
    "        self.p_of_neutral = [] \n",
    "        self.logPriorPos = 0\n",
    "        self.logPriorNeg = 0\n",
    "        self.logPriorNeu = 0\n",
    "        self.nlarge_pos = 0\n",
    "        self.nlarge_neg = 0\n",
    "        self.nlarge_neu = 0\n",
    "    def train(self,alpha,X,y):\n",
    "    #CHANGING THE COLUMN NAMES FROM 0 TO 5721 FOR EASE OF OPERATION\n",
    "        arr= np.arange(0,len(X.columns))\n",
    "        ar1 = list(arr)\n",
    "        X.columns = ar1\n",
    "        #ALSO ADDING THE LABELS TO FEATURES FOR GROUPING THE DATA\n",
    "        X['Labels'] = y\n",
    "        \n",
    "        #CALCULATING THE WORDS APPEARING IN GIVEN LABEL AND CALCULATING ALL OF THE WORDS IN A GIVEN LABEL\n",
    "        sum_of_positive = X[X['Labels'] == \"positive\"].sum(numeric_only=True)\n",
    "        sum_of_positive += alpha\n",
    "        total_sum_of_positive = sum_of_positive.sum()\n",
    "        self.nlarge_pos = sum_of_positive.nlargest(20)\n",
    "        \n",
    "        sum_of_negative = X[X['Labels'] == \"negative\"].sum(numeric_only=True)\n",
    "        sum_of_negative += alpha\n",
    "        total_sum_of_negative = sum_of_negative.sum()\n",
    "        self.nlarge_neg = sum_of_negative.nlargest(20)\n",
    "\n",
    "        sum_of_neutral = X[X['Labels'] == \"neutral\"].sum(numeric_only=True)\n",
    "        sum_of_neutral += alpha\n",
    "        total_sum_of_neutral = sum_of_neutral.sum()\n",
    "        self.nlarge_neu = sum_of_neutral.nlargest(20)\n",
    "\n",
    "        #CALCULATING THE PROBABILITY OF EACH WORD APPEARING FOR A GIVEN LABEL, THIS IS MY TRAINING DATA\n",
    "        self.p_of_positive = sum_of_positive / (total_sum_of_positive + (alpha*len(X.columns)))\n",
    "        \n",
    "        self.p_of_negative = sum_of_negative / (total_sum_of_negative + (alpha*len(X.columns)))\n",
    "        \n",
    "        self.p_of_neutral = sum_of_neutral / (total_sum_of_neutral + (alpha*len(X.columns)))\n",
    "\n",
    "        #CALCULATING THE PRIORS ACCORDING TO THE VALUE COUNT ABOVE\n",
    "        self.logPriorPos = math.log(X['Labels'].value_counts()[2]/(X['Labels'].value_counts().sum()))\n",
    "        self.logPriorNeg= math.log(X['Labels'].value_counts()[0]/(X['Labels'].value_counts().sum()))\n",
    "        self.logPriorNeut= math.log(X['Labels'].value_counts()[1]/(X['Labels'].value_counts().sum()))\n",
    "        \n",
    "        #Droping dataframe to past form to further use\n",
    "        X.drop(['Labels'], axis=1,inplace = True)\n",
    "        \n",
    "    def test(self, feat, label):\n",
    "        #TESTING FOR POSITIVE, ESTIMATING THE PROBABILITIES OF BEING POSITIVE\n",
    "        predict_pos = []\n",
    "        pre_pos_df = pd.DataFrame()\n",
    "        for index,row in feat.iterrows():\n",
    "            indexx = 0\n",
    "            for j in row:\n",
    "                #0*log0 is 0 as the assignment wants\n",
    "                if self.p_of_positive[indexx] == 0 and j == 0:\n",
    "                    predict_pos.append(0)\n",
    "                #a*log0 is -inf, and practically e^(-10000) is minus infinity\n",
    "                elif self.p_of_positive[indexx] == 0 and j != 0:\n",
    "                    predict_pos.append(-100000)\n",
    "                else:\n",
    "                    predict_pos.append((math.log(self.p_of_positive[indexx])*j ))\n",
    "                indexx +=1\n",
    "        pre_pos_df['prob'] = predict_pos\n",
    "        \n",
    "        #TESTING FOR NEUTRAL, ESTIMATING THE PROBABILITIES OF BEING NEUTRAL\n",
    "        predict_neu = []\n",
    "        pre_neu_df = pd.DataFrame()\n",
    "        for index,row in feat.iterrows():\n",
    "            indexx = 0\n",
    "            for j in row:\n",
    "                #0*log0 is 0 as the assignment wants\n",
    "                if self.p_of_neutral[indexx] == 0 and j == 0:\n",
    "                    predict_neu.append(0)\n",
    "                #a*log0 is -inf, and practically e^(-10000) is minus infinity\n",
    "                elif self.p_of_neutral[indexx] == 0 and j != 0:\n",
    "                    predict_neu.append(-100000)\n",
    "                else:\n",
    "                    predict_neu.append((math.log(self.p_of_neutral[indexx])*j))\n",
    "                indexx += 1\n",
    "        pre_neu_df['Prob'] = predict_neu\n",
    "        \n",
    "        #TESTING FOR NEGATIVE, ESTIMATING THE PROBABILITIES OF BEING NEGATIVE FOR EACH WORD\n",
    "        predict_neg = []\n",
    "        pre_neg_df = pd.DataFrame()\n",
    "        for index,row in feat.iterrows():\n",
    "            indexx = 0\n",
    "            for j in row:\n",
    "                #0*log0 is 0 as the assignment wants\n",
    "                if self.p_of_negative[indexx] == 0 and j == 0:\n",
    "                    predict_neg.append(0)\n",
    "                #a*log0 is -inf, and practically e^(-10000) is minus infinity\n",
    "                elif self.p_of_negative[indexx] == 0 and j != 0:\n",
    "                    predict_neg.append(-100000)\n",
    "                else:\n",
    "                    predict_neg.append((math.log(self.p_of_negative[indexx])*j ))\n",
    "                indexx += 1   \n",
    "        pre_neg_df['prob'] = predict_neg\n",
    "        \n",
    "        #SPLIT THE LIST ABOVE TO GET PROBABILITIES FOR EACH WORD\n",
    "        pre_split_neg = np.split(pre_neg_df,2927)\n",
    "        pre_split_pos = np.split(pre_pos_df,2927)\n",
    "        pre_split_neu = np.split(pre_neu_df,2927)\n",
    "        \n",
    "        #STORING THE ESTIMATED PROBABILITIES OF LABELS FOR EACH TWEET THIS TIME WITH SUMMING \n",
    "        #THE PROBABILITY OF EACH WORD\n",
    "        negative_prob = []\n",
    "        for i in range(len(pre_split_neg)):\n",
    "            negative_prob.append(pre_split_neg[i].sum())\n",
    "            \n",
    "        positive_prob = []\n",
    "        for i in range(len(pre_split_pos)):\n",
    "            positive_prob.append(pre_split_pos[i].sum())\n",
    "\n",
    "        neutral_prob = []\n",
    "        for i in range(len(pre_split_neu)):\n",
    "            neutral_prob.append(pre_split_neu[i].sum())\n",
    "            \n",
    "        total_pos = np.array(positive_prob)\n",
    "        total_pos += self.logPriorPos\n",
    "        total_neg = np.array(negative_prob)\n",
    "        total_neg += self.logPriorNeg\n",
    "        total_neu = np.array(neutral_prob)\n",
    "        total_neu += self.logPriorNeu\n",
    "        #CREATING A DATAFRAME FOR PREDICTED LABELS\n",
    "        self.dfScoreTable = pd.DataFrame()\n",
    "        ScoreLabel = []\n",
    "        for i in range(len(label)):\n",
    "            if (total_neg[i][0] > total_neu[i][0] and total_neg[i][0] > total_pos[i][0]):\n",
    "                ScoreLabel.append('negative')\n",
    "            elif (total_neu[i][0] >= total_neg[i][0] and total_neu[i][0] >= total_pos[i][0]):\n",
    "                ScoreLabel.append('neutral')\n",
    "            else:\n",
    "                ScoreLabel.append('positive')\n",
    "                \n",
    "        self.dfScoreTable['negative'] = ScoreLabel\n",
    "        \n",
    "        self.score = (self.dfScoreTable['negative'] == label['negative'])\n",
    "        return self.score.value_counts(1)\n",
    "    def get_20_most_used(self,vocabulary,label):\n",
    "        if label == 'positive':\n",
    "            for index, value in self.nlarge_pos.iteritems():\n",
    "                print(vocabulary.loc[index][0] )\n",
    "        elif label == 'negative':\n",
    "            for index, value in self.nlarge_neg.iteritems():\n",
    "                print(vocabulary.loc[index][0] )\n",
    "        elif label == 'neutral':\n",
    "            for index, value in self.nlarge_neu.iteritems():\n",
    "                print(vocabulary.loc[index][0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.train(alpha = 1,X = dfTrain, y= dfTrainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.749573\n",
       "False    0.250427\n",
       "Name: negative, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.test(dfTestFeature,dfTestLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.717117\n",
       "False    0.282883\n",
       "Name: negative, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb1 = MultinomialNB()\n",
    "mnb1.train(alpha=0, X=dfTrain, y=dfTrainLabel)\n",
    "mnb1.test(dfTestFeature,dfTestLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBC:\n",
    "    def __init__(self):\n",
    "        self.p_of_positive = [] \n",
    "        self.p_of_negative = [] \n",
    "        self.p_of_neutral = [] \n",
    "        self.ber_p_of_pos = []\n",
    "        self.ber_p_of_neg = []\n",
    "        self.ber_p_of_neu = []\n",
    "        self.logPriorPos = 0\n",
    "        self.logPriorNeg = 0\n",
    "        self.logPriorNeu = 0\n",
    "    def train(self,alpha,dfTrain):\n",
    "        #CHANGING THE COLUMN NAMES FROM 0 TO 5721 FOR EASE OF OPERATION\n",
    "        #CHANGING THE COLUMN NAMES FROM 0 TO 5721 FOR EASE OF OPERATION\n",
    "        arr= np.arange(0,5722)\n",
    "        ar1 = list(arr)\n",
    "        dfTrain.columns = ar1\n",
    "        #ALSO ADDING THE LABELS TO FEATURES FOR GROUPING THE DATA\n",
    "        dfTrain['Labels'] = dfTrainLabel\n",
    "        #CALCULATING THE WORDS APPEARING IN GIVEN LABEL AND CALCULATING ALL OF THE WORDS IN A GIVEN LABEL\n",
    "        BernoulliTable = dfTrain[dfTrain <= 1]\n",
    "        BernoulliTable.fillna(1, inplace = True)\n",
    "        \n",
    "        sum_of_positive = BernoulliTable[BernoulliTable['Labels'] == \"positive\"].sum(numeric_only=True)\n",
    "        sum_of_negative = BernoulliTable[BernoulliTable['Labels'] == \"negative\"].sum(numeric_only=True)\n",
    "        sum_of_neutral = BernoulliTable[BernoulliTable['Labels'] == \"neutral\"].sum(numeric_only=True)\n",
    "        \n",
    "        #CALCULATING THE PROBABILITY OF EACH WORD APPEARING FOR A GIVEN LABEL\n",
    "        self.p_of_positive = sum_of_positive / dfTrain['Labels'].value_counts()[2]\n",
    "        self.p_of_positive.fillna(0, inplace = True)\n",
    "\n",
    "        self.p_of_negative = sum_of_negative / dfTrain['Labels'].value_counts()[0]\n",
    "        self.p_of_negative.fillna(0, inplace = True)\n",
    "\n",
    "        self.p_of_neutral = sum_of_neutral / dfTrain['Labels'].value_counts()[1]\n",
    "        self.p_of_neutral.fillna(0 , inplace = True)\n",
    "\n",
    "        #CALCULATING THE PRIORS ACCORDING TO THE VALUE COUNT\n",
    "        self.logPriorPos = math.log(dfTrain['Labels'].value_counts()[2]/(dfTrain['Labels'].value_counts().sum()))\n",
    "        self.logPriorNeg= math.log(dfTrain['Labels'].value_counts()[0]/(dfTrain['Labels'].value_counts().sum()))\n",
    "        self.logPriorNeut= math.log(dfTrain['Labels'].value_counts()[1]/(dfTrain['Labels'].value_counts().sum()))\n",
    "                \n",
    "        #TO CALCULATE THE PROBABILITY 1-P\n",
    "        for index, value in self.p_of_positive.items():\n",
    "            self.ber_p_of_pos.append(1 - self.p_of_positive[index])\n",
    "            \n",
    "        for index, value in self.p_of_negative.items():\n",
    "            self.ber_p_of_neg.append(1 - self.p_of_negative[index])\n",
    "        \n",
    "        for index, value in self.p_of_neutral.items():\n",
    "            self.ber_p_of_neu.append(1 - self.p_of_neutral[index])\n",
    "\n",
    "        dfTrain.drop(['Labels'], axis=1,inplace = True)\n",
    "        \n",
    "    def test(self,dfTestFeature, dfTestLabel):\n",
    "        BernoulliTest = dfTestFeature[dfTestFeature <= 1]\n",
    "        BernoulliTest.fillna(1, inplace = True)\n",
    "        #TESTING FOR POSITIVE, ESTIMATING THE PROBABILITIES OF BEING POSITIVE\n",
    "        predict_pos = []\n",
    "        for index,row in BernoulliTest.iterrows():\n",
    "            indexx = 0\n",
    "            hold = 1\n",
    "            for j in row:\n",
    "                hold = hold *(self.p_of_positive[indexx]*j + ((1-j)*self.ber_p_of_pos[indexx]))  \n",
    "                indexx +=1\n",
    "            if hold != 0:\n",
    "                predict_pos.append(math.log(hold) + self.logPriorPos)\n",
    "            #practically e^(-10000) is minus infinity\n",
    "            else : predict_pos.append(-10000)\n",
    "        \n",
    "        #TESTING FOR NEUTRAL, ESTIMATING THE PROBABILITIES OF BEING NEUTRAL\n",
    "        predict_neu = []\n",
    "        for index,row in BernoulliTest.iterrows():\n",
    "            indexx = 0\n",
    "            hold = 1\n",
    "            for j in row:\n",
    "                hold = hold * (self.p_of_neutral[indexx]*j + ((1-j)*self.ber_p_of_neu[indexx]))\n",
    "                indexx += 1\n",
    "            if hold != 0:\n",
    "                predict_neu.append(math.log(hold) + self.logPriorNeut)\n",
    "            #practically e^(-10000) is minus infinity\n",
    "            else: predict_neu.append(-10000)\n",
    "    \n",
    "        \n",
    "        #TESTING FOR NEGATIVE, ESTIMATING THE PROBABILITIES OF BEING NEGATIVE\n",
    "        predict_neg = []\n",
    "        for index,row in BernoulliTest.iterrows():\n",
    "            indexx = 0\n",
    "            hold = 1\n",
    "            for j in row:\n",
    "                hold = hold * (self.p_of_negative[indexx]*j + ((1-j)*self.ber_p_of_neg[indexx]))\n",
    "                indexx += 1\n",
    "            if hold != 0:\n",
    "                predict_neg.append(math.log(hold) + self.logPriorNeg)\n",
    "            #practically e^(-10000) is minus infinity\n",
    "            else: predict_neg.append(-10000)\n",
    "            \n",
    "        #CREATING A DATAFRAME FOR PREDICTED LABELS\n",
    "        dfScoreTable = pd.DataFrame()\n",
    "        ScoreLabel = []\n",
    "        for i in range(len(dfTestLabel)):\n",
    "            if (predict_neg[i] > predict_neu[i] and predict_neg[i] > predict_pos[i]):\n",
    "                ScoreLabel.append('negative')\n",
    "            elif (predict_neu[i] >= predict_neg[i] and predict_neu[i] >= predict_pos[i]):\n",
    "                ScoreLabel.append('neutral')\n",
    "            else:\n",
    "                ScoreLabel.append('positive')\n",
    "\n",
    "        dfScoreTable['negative'] = ScoreLabel\n",
    "\n",
    "        #COMPARING THE PREDICTED LABELS WITH REAL ONES AND PASS IT INTO ANOTHER OBJECT, COMPARING WILL \n",
    "        #RETURN TRUE OR FALSE AT EACH ROW\n",
    "        score = dfScoreTable['negative'] == dfTestLabel['negative']\n",
    "        return score.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliBC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.train(alpha = 0,dfTrain = dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.640929\n",
       "False    0.359071\n",
       "Name: negative, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb.test(dfTestFeature, dfTestLabel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>flight</td>\n",
       "      <td>3948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>@united</td>\n",
       "      <td>3890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>@usairways</td>\n",
       "      <td>2998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>@americanair</td>\n",
       "      <td>2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>@southwestair</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>@jetblue</td>\n",
       "      <td>2221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>cancelled</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>service</td>\n",
       "      <td>963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>help</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time</td>\n",
       "      <td>791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>customer</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>hours</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>flights</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>hold</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>plane</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>delayed</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>gate</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@virginamerica</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>call</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>flightled</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0     1\n",
       "16            flight  3948\n",
       "572          @united  3890\n",
       "2151      @usairways  2998\n",
       "896     @americanair  2960\n",
       "300    @southwestair  2455\n",
       "301         @jetblue  2221\n",
       "309        cancelled  1065\n",
       "157          service   963\n",
       "76              help   872\n",
       "21              time   791\n",
       "265         customer   753\n",
       "140            hours   684\n",
       "161          flights   648\n",
       "247             hold   644\n",
       "278            plane   631\n",
       "306          delayed   545\n",
       "408             gate   519\n",
       "0     @virginamerica   517\n",
       "155             call   515\n",
       "332        flightled   508"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FINDING THE MOST USED 20 WORDS IN TWEETS\n",
    "dfVocabulary = pd.read_csv('question-4-vocab.txt',sep='\\t',header= -1)\n",
    "dfVocabulary.nlargest(20,columns = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@jetblue\n",
      "@united\n",
      "@southwestair\n",
      "flight\n",
      "@usairways\n",
      "@virginamerica\n",
      "flights\n",
      "help\n",
      "fleek\n",
      "fleet's\n",
      "dm\n",
      "time\n",
      "tomorrow\n",
      "flying\n",
      "cancelled\n",
      "fly\n",
      "change\n",
      "today\n",
      "travel\n",
      "check\n"
     ]
    }
   ],
   "source": [
    "mnb.get_20_most_used(vocabulary = dfVocabulary, label='neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@united\n",
      "flight\n",
      "@usairways\n",
      "@southwestair\n",
      "@jetblue\n",
      "cancelled\n",
      "service\n",
      "hours\n",
      "hold\n",
      "time\n",
      "customer\n",
      "help\n",
      "delayed\n",
      "plane\n",
      "hour\n",
      "flights\n",
      "bag\n",
      "gate\n",
      "late\n",
      "flightled\n"
     ]
    }
   ],
   "source": [
    "mnb.get_20_most_used(vocabulary = dfVocabulary, label='negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@southwestair\n",
      "@jetblue\n",
      "@united\n",
      "flight\n",
      "@usairways\n",
      "great\n",
      "@virginamerica\n",
      "service\n",
      "love\n",
      "best\n",
      "guys\n",
      "customer\n",
      "time\n",
      "awesome\n",
      "help\n",
      "airline\n",
      "amazing\n",
      "today\n",
      "fly\n",
      "flying\n"
     ]
    }
   ],
   "source": [
    "mnb.get_20_most_used(vocabulary = dfVocabulary, label='positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahir\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb.fit(dfTrain,dfTrainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7529894089511445"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb.score(dfTestFeature,dfTestLabel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
